# rStar-Math, el AlphaGo de las matemáticas

- [<img src="../../res/spotify-icon-256.webp" alt="spotify_logo" width="32" style="position: relative; top: 5px;"> Spotify](https://open.spotify.com/episode/1CuV1w5rAAcJf5oPjVxCF0?si=sX2Cg9dzTCqbXGVCPioUeA)
- [<img src="../../res/youtube-icon-256.png" alt="youtube_logo" width="32" style="position: relative; top: 10px;"> Youtube](https://youtu.be/fu0i_E1jQTk)
- [<img src="../../res/ivoox-icon-256.webp" alt="ivoox_logo" width="32" style="position: relative; top: 5px;"> Ivoox](https://go.ivoox.com/rf/139026905)
- [<img src="../../res/apple-icon-256.webp" alt="apple_logo" width="32" style="position: relative; top: 5px;"> Apple Podcasts](https://podcasts.apple.com/us/podcast/rstar-math-el-alphago-de-las-matem%C3%A1ticas/id1669083682?i=1000687158065)

¿Pueden los modelos pequeños mostrar capacidades de razonamiento matemático comparables a o1? En Microsoft creen que sí y nos lo demuestran con un método inspirado en AlphaGo, el sistema que venció a Lee Sedol hace ya casi una década. Hoy en la tertulia vemos modelos de lenguaje pequeños que superan a o1.

Participan en la tertulia: Paco Zamora, Íñigo Olcoz, Carlos Larríu, Íñigo Orbegozo y Guillermo Barbadillo.

Recuerda que puedes enviarnos dudas, comentarios y sugerencias en: <https://twitter.com/TERTUL_ia>