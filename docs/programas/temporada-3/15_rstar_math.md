# rStar-Math, el AlphaGo de las matemáticas

- [<img src="https://cdn.iconscout.com/icon/free/png-256/spotify-36-721973.png" alt="spotify_logo" width="32" style="position: relative; top: 5px;"> Spotify]()
- [<img src="https://cdn.icon-icons.com/icons2/195/PNG/256/YouTube_23392.png" alt="youtube_logo" width="32" style="position: relative; top: 10px;"> Youtube](https://youtu.be/fu0i_E1jQTk)
- [<img src="https://i0.wp.com/parqueeste.org/wp-content/uploads/2020/07/ivoox-icon.png?fit=256%2C256&ssl=1" alt="ivoox_logo" width="32" style="position: relative; top: 5px;"> Ivoox](https://go.ivoox.com/rf/139026905)
- [<img src="https://cdn.iconscout.com/icon/free/png-256/apple-853-675472.png" alt="apple_logo" width="32" style="position: relative; top: 5px;"> Apple Podcasts]()

¿Pueden los modelos pequeños mostrar capacidades de razonamiento matemático comparables a o1? En Microsoft creen que sí y nos lo demuestran con un método inspirado en AlphaGo, el sistema que venció a Lee Sedol hace ya casi una década. Hoy en la tertulia vemos modelos de lenguaje pequeños que superan a o1.

Participan en la tertulia: Paco Zamora, Íñigo Olcoz, Carlos Larríu, Íñigo Orbegozo y Guillermo Barbadillo.

Recuerda que puedes enviarnos dudas, comentarios y sugerencias en: <https://twitter.com/TERTUL_ia>